{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Introduction\n",
    "\n",
    "![image](../../images/pandas_logo.png)\n",
    "\n",
    "**Pandas** is probably the most popular Python library for data manipulation and analysis. Pandas is designed to work with a wide range of data sources, including: `CSV`, `Excel`, `Parquet`, `Pickle`, `JSON`, and many more.\n",
    "\n",
    "**NOTE**: Data Scientists / Data Professionals ðŸ’š **Pandas**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Use Pandas for the first time\n",
    "\n",
    "`pandas` should be automatically installed as part of `anaconda`. Nonetheless, if for some reasons it is missing, you can enter the following command in a `anaconda prompt` or a `terminal` to install `pandas`:\n",
    "\n",
    "```bash\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "You can verify that it is installed by entering the following codes in a cell in a notebook:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"The version of pandas is: {pandas.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"pandas is not installed!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely common to use `pd` as the alias for `pandas` in `Python`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Objects of Pandas\n",
    "\n",
    "There are two **core objects** of `pandas`:\n",
    "- **DataFrame**: A table-like object that can be easily created and manipulated.\n",
    "- **Series**: A 1-dimensional sequence of data values. If a **dataframe** is a table, then each column is a **Series**.\n",
    "\n",
    "![image](../../images/df_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **create a dataframe**, you can use `pd.DataFrame()` constructor.\n",
    "\n",
    "Let's create the dataframe we see in the image above and assign it to a variable called `df`.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    \"Height\": [1.60, 1.76, 1.95],\n",
    "    \"Weight\": [60.0, 56.0, 85.0],\n",
    "})\n",
    "\n",
    "df\n",
    "```\n",
    "\n",
    "As you can see, we provide a dictionary of **column names** and **column values** to the `pd.DataFrame()` constructor. The **column names** are the **keys** of the dictionary, and the **column values** are the **values** of the dictionary. This is a very common way to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers in the far left of the dataframe are indices. The indices are automatically assigned by `pandas` and are called **row labels**.\n",
    "\n",
    "In this example, the indices are `0`, `1`, and `2`. The [`1`, `Weight`] entry contains the value `56.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "- Dataframes can contain as many columns and rows as you want. The entries in the dataframes are not limited to numbers, and they can be of any type.\n",
    "- Dataframes are **mutable**. This means that you can change the values of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Given the following dictionary, can you create a dataframe that looks like the one below?\n",
    "\n",
    "```python\n",
    "dict_data = {\n",
    "    \"Name\": [\"Millie Bobby Brown\", \"Finn Wolfhard\", \"Natalie Dyer\", \"Kaley Cuoco\", \"Jim Parsons\"],\n",
    "    \"Age\": [18, 19, 27, 36, 49],\n",
    "    \"Height\": [1.60, 1.80, 1.63, 1.68, 1.86],\n",
    "    \"TV Series\": [\"Stranger Things\", \"Stranger Things\", \"Stranger Things\", \"Big Bang Theory\", \"Big Bang Theory\"],\n",
    "}\n",
    "```\n",
    "\n",
    "|Name|Age|Height|TV_Series|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|Millie Bobby Brown|18|1.60|Stranger Things|\n",
    "|Finn Wolfhard|19|1.80|Stranger Things|\n",
    "|Natalie Dyer|27|1.63|Stranger Things|\n",
    "|Kaley Cuoco|36|1.68|Big Bang Theory|\n",
    "|Jim Parsons|49|1.86|Big Bang Theory|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using generic numbers as **row labels**, can we create a dataframe having row labels to be the names of the people in the dataframe?\n",
    "\n",
    "The answer is **YES**, and it's very easy.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\n",
    "    \"Age\": [18, 19, 27, 36, 49],\n",
    "    \"Height\": [1.60, 1.80, 1.63, 1.68, 1.86],\n",
    "    \"TV Series\": [\"Stranger Things\", \"Stranger Things\", \"Stranger Things\", \"Big Bang Theory\", \"Big Bang Theory\"],\n",
    "}, index=[\"Millie Bobby Brown\", \"Finn Wolfhard\", \"Natalie Dyer\", \"Kaley Cuoco\", \"Jim Parsons\"])\n",
    "\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we only have to provide the list of names as values of the `index` argument in the `pd.DataFrame()` constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "\n",
    "In order to **create a series**, you can use `pd.Series()` constructor.\n",
    "\n",
    "```python\n",
    "height_series = pd.Series([1.60, 1.80, 1.63, 1.68, 1.86])\n",
    "```\n",
    "\n",
    "As mentioned above, a series can be considered as a column in a dataframe. \n",
    "- A series can contain as many entries as you want.\n",
    "- A series also has `index` that are automatically generated by `pandas` but can be changed as you wish. \n",
    "- A series can be of any type.\n",
    "- A series has one overall `name` that is used to identify the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate height_series, but we'll give it a `name` of `Height` this time.\n",
    "\n",
    "```python\n",
    "height_series = pd.Series([1.60, 1.80, 1.63, 1.68, 1.86], name=\"Height\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recreate height_series but we'll give it a name this time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from file\n",
    "\n",
    "It's good that you know how to create a dataframe/series from scratch. In real life, you will probably be doing that only when you want to create some very small tests specific to your need. In this section, we'll learn how to use `pandas` to read data from a **CSV file** and store the data in a dataframe.\n",
    "\n",
    "**CSV file** is a common file format used to store data. It is a simple text file that contains a list of comma separated values.\n",
    "\n",
    "In the `data` folder, I have already provided a CSV file called `weatherAUS.csv`. We'll be using this data for the rest of the lesson.\n",
    "\n",
    "![image](../../images/data_tree.png)\n",
    "\n",
    "The **Weather in Australia** dataset is a public dataset. More information about the **Weather in Australia** dataset is available here [here](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package).\n",
    "\n",
    "**NOTE**: **Kaggle** is probably the best source of learning for all **data enthusiasts**.\n",
    "\n",
    "The data contains daily weather observations taken from various weather stations in Australia from `1st November 2007` to `25th June 2017`.\n",
    "\n",
    "The data includes `145k` rows of weather observation data. Each row represents a unique observation set and each observation set contains the attributes shown in the table below\n",
    "\n",
    "|No|Attribute|Description|Type|\n",
    "|-|-|-|-|\n",
    "|1|Date|The date of observation|Data time|\n",
    "|2|Location|The common name of the location of the weather station|String|\n",
    "|3|MinTemp|Minimun temperature in $^\\circ C$|Float|\n",
    "|4|MaxTemp|Maximum temperature in $^\\circ C$|Float|\n",
    "|5|Rainfall|Amount of rainfall for the day in $mm$|Float|\n",
    "|6|Evaporation|class A pan evaporation in $mm$ in the 24 hours to 9am|Float|\n",
    "|7|Sunshine|Number of hours of bright sunshine in the day|Float|\n",
    "|8|WindGustDir|Direction of the strongest wind gust in the 24 hours to midnight|String|\n",
    "|9|WindGustSpeed|Speed in $Km/hr$ of the strongest wind gust in the 24 hours to midnight|Float|\n",
    "|10|WindDir9am|Direction of the wind at 9am|String|\n",
    "|11|WindDir3pm|Direction of the wind at 3pm|String|\n",
    "|12|WindSpeed9am|Wind speed in $km/hr$ averaged over 10 minutes prior to 9am|Float|\n",
    "|13|WindSpeed3pm|Wind speed in $km/hr$ averaged over 10 minutes prior to 3pm|Float|\n",
    "|14|Humidity9am|Humidity in percentage at 9am|Float|\n",
    "|15|Humidity3pm|Humidity in percentage at 3pm|Float|\n",
    "|16|Pressure9am|Atmospheric pressure in $hpa$ reduced to mean sea level at 9am|Float|\n",
    "|17|Pressure3pm|Atmospheric pressure in $hpa$ reduced to mean sea level at 3pm|Float|\n",
    "|18|Cloud9am|Fraction of sky obscured by cloud at 9am, measured in $oktas$ in a range 0 (sky clear) to 8 (sky completely overcast)|Float|\n",
    "|19|Cloud3pm|Fraction of sky obscured by cloud at 3pm, measured in $oktas$ in a range 0 (sky clear) to 8 (sky completely overcast)|Float|\n",
    "|20|Temp9am|Temperature in $^\\circ C$ at 9am|Float|\n",
    "|21|Temp3pm|Temperature in $^\\circ C$ at 3pm|Float|\n",
    "|22|RainToday|Boolean attribute. 'Yes' if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 'No'|String|\n",
    "|23|RainTomorrow|The target variable. Did it rain tomorrow? Boolean. 'Yes' if we predict rain for tomorrow, otherwise 'No'|String|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data from the CSV file and play with the data.\n",
    "\n",
    "```python\n",
    "weather_data = pd.read_csv(\"../../data/weatherAUS.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `np.ndarray`, `pd.DataFrame` also has `shape` attribute that can be used to check the number of rows and columns in the dataframe.\n",
    "\n",
    "```python\n",
    "weather_data.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that the dataframe has `145,460` rows and `23` columns.\n",
    "\n",
    "In order to examine the data, we can use the `head()`, `tail()`, or `sample()` method.\n",
    "- `head(n)` returns the first `n` rows of the dataframe.\n",
    "- `tail(n)` returns the last `n` rows of the dataframe.\n",
    "- `sample(n)` returns `n` random rows of the dataframe.\n",
    "\n",
    "Let's view the first `5` rows of the `weather_data` dataframe.\n",
    "\n",
    "```python\n",
    "weather_data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick observation**:\n",
    "- You can see that there are lots of values `NaN` in the dataframe. `NaN` stands for `Not a Number`, and it means that the value is not available. We'll learn how to deal with `NaN` values later.\n",
    "- The dataframe is also **too long** to be displayed in a single screen. That's why we see a `...` section in the middle of the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you get 10 random rows from the `weather_data` dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `np.ndarray`, `pd.DataFrame` can also be tranposed. This is useful when you want to display the dataframe in a more readable format.\n",
    "\n",
    "Let's view the transposed version of the first 5 rows.\n",
    "\n",
    "```python\n",
    "weather_data.head().T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing, Selecting, and Assigning in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **several ways** to access a column in the dataframe.\n",
    "- `.` operator: `df.column_name` returns a series with the values of the column `column_name`.\n",
    "- `[]` (indexing) operator: `df[\"column_name\"]` does the same thing as the above.\n",
    "\n",
    "Let's access the `Location` column of the `weather_data` dataframe.\n",
    "\n",
    "```python\n",
    "weather_data.Location\n",
    "weather_data[\"Location\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I prefer to use the indexing syntax (`df[\"column_name\"]`) because it is more readable and extremely clear that we are accessing a column from the dataframe. In addition, the indexing syntax can handle cases where the column name contains reserved characters like `spaces`.\n",
    "\n",
    "For example, `df[\"column name\"]` will work, but `df.column name` will not.\n",
    "\n",
    "`pandas` has built-in accessor operators called `iloc` and `loc` for more advanced operations.\n",
    "- `iloc` is **position-based** selection, while `loc` is **label-based** selection.\n",
    "- `iloc` is short for `integer location` and is used to select rows and columns by their index (position).\n",
    "- Both `iloc` and `loc` are **row-first, column-second**. \n",
    "\n",
    "Let's access the first row of the `weather_data` dataframe using `iloc`.\n",
    "\n",
    "```python\n",
    "weather_data.iloc[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access the `MaxTemp` column of `weather_data` using the `iloc` accessor.\n",
    "\n",
    "```python\n",
    "weather_data.iloc[:, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Do you notice something similar to what we learnt in the previous lessons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">[TODO] ðŸ“–</font>\n",
    "\n",
    "It's the slicing operator (`:`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you select the first 10 rows of the column `MinTemp` from the `weather_data` dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access the first row of the `weather_data` dataframe using `loc`.\n",
    "\n",
    "```python\n",
    "weather_data.loc[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look that different from `iloc` accessor right? In fact, `iloc` is conceptually simpler than `loc` because it ignores the dataset's indices. When we're using `iloc`, we only need to specify the index of the row(s) and/or column(s) we want to access. `loc` is more flexible because it allows us to specify the label of the row(s) and/or column(s) we want to access.\n",
    "\n",
    "For example, let's access the first 5 rows from the following columns (`MinTemp`, `MaxTemp`, `RainToday`, `RainTomorrow`) of the `weather_data` dataframe.\n",
    "\n",
    "```python\n",
    "weather_data.loc[:4, [\"MinTemp\", \"MaxTemp\", \"RainToday\", \"RainTomorrow\"]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything different about `loc` and `iloc` in terms of the provided indices? \n",
    "\n",
    "`iloc[:5]` is equivalent to `loc[:4]`. The index in `loc` is inclusive while `iloc` is exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an even **more compact** way to select a subset dataframe from the original dataframe using the `[]` operator. Personally, I prefer to use this method as it is highly readable and easy to use.\n",
    "\n",
    "Let's do the same using `[]` operator!\n",
    "\n",
    "```python\n",
    "weather_data[[\"MinTemp\", \"MaxTemp\", \"RainToday\", \"RainTomorrow\"]].head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when creating a dataframe, we have an option to specify the `index` of the dataframe instead of using the automatically generated numbers. Thus, in this case, if we want to use `Location` column as the `index` of our `weather_data` dataframe, we could use the `set_index()` method. \n",
    "\n",
    "```python\n",
    "weather_data.set_index(\"Location\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting\n",
    "\n",
    "This will be extremely similar to how we filter data in a `numpy` array. Often in a project, we want to select a subset of the data based on certain conditions to perform some analysis in depth.\n",
    "\n",
    "For example, let's select all records where the `Location` is `Sydney`.\n",
    "\n",
    "```python\n",
    "weather_data[weather_data[\"Location\"] == \"Sydney\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is a dataframe with only the records where the `Location` is `Sydney`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "How many rows are there where the `Location` is `Sydney` and the `RainTomorrow` is `Yes`?\n",
    "\n",
    "**Hint**: We can use `&` (`ampersand`: on a high level, it's similar to `and`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "How many rows are there where the `Evaporation` is greater than `3` or `RainTomorrow` is `Yes`?\n",
    "\n",
    "**Hint**: We can use `|` (`pipe`: on a high level, it's similar to `or`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you select all records where the `Location` is either `Sydney`, `Melbourne`, or `Canberra`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `|` operator works but it's quite cumbersome for that scenario. There's a better way to do the same thing by using the `pandas` `isin` method. `isin` lets you select data whose value **is in** a list of values.\n",
    "\n",
    "```python\n",
    "weather_data[weather_data[\"Location\"].isin([\"Sydney\", \"Melbourne\", \"Canberra\"])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have seen lots of `NaN` values above. How do we filter out the records with missing values? We can use the `pandas` `isnull` method. The opposite of `isnull` is `notnull`. We can also use `~` (`tilde`) to invert the result.\n",
    "\n",
    "```python\n",
    "weather_data[weather_data[\"Evaporation\"].isnull()]\n",
    "weather_data[~weather_data[\"Evaporation\"].isnull()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you check if the sum of rows of the 2 dataframes we got above is the same as the number of rows in the `weather_data` dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning\n",
    "\n",
    "Assigning data to a dataframe is very simple and very similar to how we add a new `key-value` pair to an existing dictionary.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Can you create a column called `MinTemp > 5` to indicate whether the `MinTemp` is greater than 5 or not? The column will contain value \"Yes\" if the `MinTemp` is greater than 5, and \"No\" otherwise.\n",
    "\n",
    "**Hint**: Use `np.where()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Functions and Maps\n",
    "\n",
    "## Summary Functions\n",
    "`pandas` has functions like `describe()` and `info()` which provide a summary of the data. They can be used to give you an overview of the data by displaying relevant descriptive statistics of the data.\n",
    "\n",
    "Let's see how `describe()` works!\n",
    "\n",
    "Note that when the dataframe has too many columns that can't be easily displayed in a single screen, we can transpose the result to view it better.\n",
    "\n",
    "```python\n",
    "weather_data.describe().T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe()` gives you the following information.\n",
    "- `count`: the number of `notnull` rows for that particular column.\n",
    "- `mean`: the mean of the column.\n",
    "- `std`: the standard deviation of the column.\n",
    "- `min`: the minimum value of the column.\n",
    "- `25%`: the 25th percentile of the column.\n",
    "- `50%`: the 50th percentile of the column.\n",
    "- `75%`: the 75th percentile of the column.\n",
    "- `max`: the maximum value of the column.\n",
    "\n",
    "If you use `describe()` as is, you will only get summary statistics for the **numerical** columns. If you want to get the summary statistics for all columns, you can use the `.describe(include=\"all\")` method.\n",
    "\n",
    "```python\n",
    "weather_data.describe(include=\"all\").T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually extremely hard to read, so normally, we view the summary statistics of **numerical** and **categorical** columns separately.\n",
    "\n",
    "To view the summary statistics of only **categorical** columns, we can use the `.describe(include=\"object\")` method.\n",
    "- `count`: the number of `notnull` rows for that particular column.\n",
    "- `unique`: the number of unique values in the column.\n",
    "- `top`: the top most common values in the column.\n",
    "- `freq`: the frequency of the top most common values.\n",
    "\n",
    "```python\n",
    "weather_data.describe(include=\"object\").T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how `info()` works!\n",
    "- `Non-Null Count`: the number of `notnull` rows for that particular column.\n",
    "- `Dtype`: the data types of the columns.\n",
    "\n",
    "`info()` gives us an idea to see whether the `pd.read_csv()` method used before has inferred the data types of the columns correctly. We can easily assign the correct data types to each column while reading the data.\n",
    "\n",
    "```python\n",
    "weather_data.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the data type of a single column, we can use the `.dtype` attribute.\n",
    "\n",
    "```python\n",
    "weather_data[\"Location\"].dtype\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly view the data types of each column, we can use the `.dtypes` attribute.\n",
    "\n",
    "```python\n",
    "weather_data.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding unique values in a column, we can use:\n",
    "- `unique()` to get the list of unique values in the column of interest.\n",
    "- `nunique()` to get the number of unique values in the column of interest.\n",
    "- `value_counts()` to get the frequency of each unique value in the column of interest.\n",
    "\n",
    "```python\n",
    "weather_data[\"Location\"].unique()\n",
    "weather_data[\"Location\"].nunique()\n",
    "weather_data[\"Location\"].value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps\n",
    "\n",
    "**Mapping** is a way to map one set of values to another set of values. This is one of the way to transform the raw data to the desired format. This is something that data scientists do a lot.\n",
    "\n",
    "There are lots of mapping methods in `pandas`, but you'll often use the `map()` method.\n",
    "\n",
    "In data science, it's very common to standardise features to a common scale. One of the most common scaler is called **Standard Scaler** where we transform each data point of the feature into the `z-score`. \n",
    "\n",
    "z = $\\displaystyle \\frac{x - mean}{std}$\n",
    "\n",
    "Let's see how we normalise the `Rainfall` column using `map()` method! We'll also save the normalised data to a new column called `NormalisedRainfall`.\n",
    "\n",
    "```python\n",
    "mean_rainfall = weather_data[\"Rainfall\"].mean()\n",
    "std_rainfall = weather_data[\"Rainfall\"].std()\n",
    "\n",
    "weather_data[\"NormalisedRainfall\"] = weather_data[\"Rainfall\"].map(\n",
    "    lambda x: (x - mean_rainfall) / std_rainfall\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the usage of the anonymous (`lambda`) function. The `lambda` function passed to `map()` expects a single value from the Series (a point value, in the above example), and return a transformed version of that value. `map()` returns a new Series where all the values have been transformed by the anonymous function.\n",
    "\n",
    "Let's look at the normalised column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we don't even need to use `map()` to normalise the data. There are plenty of common mapping operations already built-in `pandas`.\n",
    "\n",
    "Let's do the same thing in a faster way!\n",
    "\n",
    "```python\n",
    "mean_rainfall = weather_data[\"Rainfall\"].mean()\n",
    "std_rainfall = weather_data[\"Rainfall\"].std()\n",
    "\n",
    "weather_data[\"NormalisedRainfall_v2\"] = (weather_data[\"Rainfall\"] - mean_rainfall)/std_rainfall\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `NormalisedRainfall_v2` is exactly the same as `NormalisedRainfall`. In fact, `NormalisedRainfall_v2` is even calculated faster. Let's verify it by using `%timeit` magic command.\n",
    "\n",
    "```python\n",
    "%timeit weather_data[\"NormalisedRainfall\"] = weather_data[\"Rainfall\"].map(lambda x: (x - mean_rainfall) / std_rainfall)\n",
    "%timeit weather_data[\"NormalisedRainfall_v2\"] = (weather_data[\"Rainfall\"] - mean_rainfall)/std_rainfall\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this, `map()` method is still very useful as it is a lot more flexible than the built-in operations. You can use `map()` to apply conditional logics, which can't be done using the built-in operations.\n",
    "\n",
    "There is another very common mapping method called `apply()`. It is similar to `map()` but it works on a DataFrame instead of a Series.\n",
    "\n",
    "Let's say we want to apply the same standardisation to the following columns (\"Rainfall\", \"Temp9am\", \"WindGustSpeed\") in the `weather_data` dataframe!\n",
    "\n",
    "```python\n",
    "df = weather_data[[\"Rainfall\", \"Temp9am\", \"WindGustSpeed\"]]\n",
    "\n",
    "def standardise(df):\n",
    "    mean_column = df.mean()\n",
    "    std_column = df.std()\n",
    "    return (df - mean_column) / std_column\n",
    "\n",
    "standardised_df = df.apply(standardise, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare `weather_data[\"NormalisedRainfall\"]` and `standardised_df[\"Rainfall\"]` to see that we have the same result!\n",
    "\n",
    "To compare 2 `pd.Series`, we can use the `equals()` method. More about the `equals()` method is available [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.equals.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and Sorting\n",
    "\n",
    "## Grouping\n",
    "It is very common in data science that we need to aggregate the data based on some criteria and perform some calculations on grouped data. In order to do so, we can use the `groupby()` method.\n",
    "\n",
    "How `groupby()` works is as follows:\n",
    "\n",
    "![image](../../images/pd_groupby.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the total `Rainfall` for each `Location` in the `weather_data` dataframe! We'll assign the result to a variable called `total_rainfall_by_location`.\n",
    "\n",
    "```python\n",
    "total_rainfall_by_location = weather_data.groupby([\"Location\"])[\"Rainfall\"].sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the result is a `Series` with the `Location` as the index and the total `Rainfall` as the value. Often, we want to have the result as a dataframe instead of a `Series`. In order to do that, we can use the `reset_index()` method, which will reset the index of the result `Series` and allow us to have `Location` as another column in the result dataframe.\n",
    "\n",
    "```python\n",
    "total_rainfall_by_location = total_rainfall_by_location.reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could combine everything in 1 single line as follows:\n",
    "\n",
    "```python\n",
    "total_rainfall_by_location = weather_data.groupby([\"Location\"])[\"Rainfall\"].sum().reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Can you find the maximum `MinTemp` for each location in the `weather_data` dataframe?\n",
    "1. Based on the result above, can you find the location with the highest `MinTemp`?\n",
    "\n",
    "**Hint**: You can use the `max()` method to find the maximum value in a `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many aggregate functions in `pandas`. The most common ones that you'll use are: `sum()`, `mean()`, `min()`, `max()`, `median()`. If you want to perform multiple aggregate functions on a single column, you can use the `agg()` method.\n",
    "\n",
    "For example, we can write 1 single line of code to find the following information about the `Rainfall` column:\n",
    "- Total `Rainfall` per `Location`.\n",
    "- Average `Rainfall` per `Location`.\n",
    "- Minimum `Rainfall` per `Location`.\n",
    "- Maximum `Rainfall` per `Location`.\n",
    "\n",
    "```python\n",
    "weather_data.groupby([\"Location\"])[\"Rainfall\"].agg([\"sum\", \"mean\", \"min\", \"max\"]).reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the column names are automatically generated by the `agg()` method. Usually, it's a good idea to rename the columns to make them more readable. This is because if we perform the same set of functions on another column (e.g. `MaxTemp`), the column names will be the same as the aggregated functions, and it will be hard to combine the results and make sense of the data.\n",
    "\n",
    "We can use the `rename()` method to rename dataframe columns. \n",
    "- It's a very useful method as we can rename multiple columns at once by supplying a dictionary of old and new column names to the method. \n",
    "- We can choose to have the operation done `inplace` or `not inplace`.\n",
    "    - `inplace` means we will be saving the new result in the original dataframe.\n",
    "    - `not inplace` means we will be creating a new dataframe with the new result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the column names `not inplace` first and understand what it does.\n",
    "\n",
    "```python\n",
    "result.rename(columns={\n",
    "    \"sum\": \"TotalRainfall\",\n",
    "    \"mean\": \"AverageRainfall\",\n",
    "    \"min\": \"MinRainfall\",\n",
    "    \"max\": \"MaxRainfall\"\n",
    "}, inplace=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we access `result` variable, we can see that it remains intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the column names `inplace` this time!\n",
    "\n",
    "```python\n",
    "result.rename(columns={\n",
    "    \"sum\": \"TotalRainfall\",\n",
    "    \"mean\": \"AverageRainfall\",\n",
    "    \"min\": \"MinRainfall\",\n",
    "    \"max\": \"MaxRainfall\"\n",
    "}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `result` dataframe is now changed with the new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example thus far, we have only grouped by 1 single column (e.g. `Location`). In fact, we can group by multiple columns as well.\n",
    "\n",
    "Remember that we have created a column called `MinTemp > 5` above which has the value of `Yes` if the `MinTemp` is greater than 5 and `No` otherwise.\n",
    "\n",
    "Let's find the total `Rainfall` for each `Location` and `MinTemp > 5` in the `weather_data` dataframe! We'll assign the result to a variable called `result`.\n",
    "\n",
    "```python\n",
    "result = weather_data.groupby([\"Location\", \"MinTemp > 5\"])[\"Rainfall\"].sum().reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple aggregation can already tell us **some interesting information about the data**. You can see that in any `Location` if `MinTemp` is greater than 5, there's a higher chance that it will rain (`Rainfall` is higher) than if `MinTemp` is less than or equal to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to perform some operations on the `Date` column of the `weather_data` dataframe to extract the `Month` and `Year` from the `Date` column. This is the reason why we need to be very careful when reading in `datetime` information. Most of the time, we want to ensure that the `Date` column is recognised as actual `datetime` instead of a string.\n",
    "\n",
    "```python\n",
    "weather_data[\"Month\"] = weather_data[\"Date\"].dt.month\n",
    "weather_data[\"Year\"] = weather_data[\"Date\"].dt.year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with `datetime` is fun but a bit tricky. Don't worry about it too much at the moment! \n",
    "\n",
    "[Here](https://www.dataquest.io/blog/datetime-in-pandas/) is a pretty good tutorial on how to start working with `datetime` in `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you view 10 random samples `[\"Date\", \"Month\", \"Year\"]` from the `weather_data` dataframe to see if we have performed the operations correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you tell me which `Month` has the highest `Rainfall` in `Sydney`?\n",
    "\n",
    "**Hint**: You need to filter for `Location == Sydney` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "How many `Date`s are there for each `Year` for `Sydney`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there're some gaps in the data. For instance, `2008`, `2011`, `2012`, and `2013` didn't contain 1-full year worth of data. `2017` is an exception because we already know the data is only collected until `Jun 2017`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you tell me which `Year` has the highest amount of `Rainfall` in `Adelaide`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "Sorting is a very common operation in data/programming. We can sort a `Series` or a `DataFrame` by using the `sort_values()` method.\n",
    "\n",
    "How sorting works together with `groupby()` is as follows:\n",
    "\n",
    "![image](../../images/pd_groupby_sort.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the `total_rainfall_by_location` dataframe we created earlier. Let's try to sort that dataframe by `Rainfall` in ascending order.\n",
    "\n",
    "```python\n",
    "total_rainfall_by_location = weather_data.groupby([\"Location\"])[\"Rainfall\"].sum().reset_index()\n",
    "total_rainfall_by_location = total_rainfall_by_location.sort_values(by=\"Rainfall\", ascending=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you sort the `total_rainfall_by_location` dataframe by `Rainfall` in descending order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "\n",
    "As we have seen, there are lots of `NaN` values in the data. We have also learnt to use the `isnull()` and `notnull()` methods to check for missing values. \n",
    "\n",
    "**Dealing missing values** is something that data professionals do all the time. There are many ways to deal with missing values.\n",
    "- Replace the missing values with some value (e.g. `mean`, `median` of the numerical columns or \"Unknown\" for categorical columns).\n",
    "- Drop the rows with the missing values.\n",
    "\n",
    "We'll learn about 2 very useful methods for dealing with missing values here: `fillna()` and `dropna()`.\n",
    "- `fillna()`, as the name suggests`, will fill the missing values with a specified value.\n",
    "- `dropna()`, as the name suggests`, will drop the rows with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happen if we drop all rows with at least 1 missing value.\n",
    "\n",
    "```python\n",
    "weather_data_droppedna = weather_data.dropna()\n",
    "\n",
    "print(f\"Original shape of data: {weather_data.shape}\")\n",
    "print(f\"New shape of data: {weather_data_droppedna.shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shrinks significantly (about 3 times) because we have dropped all rows with at least 1 missing value. This is generally **NOT A GOOD IDEA** because we lose too much valuable information. Normally, we only drop rows with missing values if we are very sure that many columns in the same rows are also missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you tell me which column(s) have the most missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Let's choose column `Pressure9am` as an example and fill in the missing values using the `fillna()` method.\n",
    "\n",
    "In order to decide which value to fill in, let's take a look at the summary statistics of that column.\n",
    "\n",
    "Can you do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose to fill the missing values of `Pressure9am` with the `mean` of that column. Normally, I prefer to use `median` since `mean` is most often skewed by outliers. Nonetheless, in this case, the `mean` and `median` are very close to each other so we can use `mean` here.\n",
    "\n",
    "We also choose to perform the operations `inplace` which means that we will modify the original dataframe.\n",
    "\n",
    "```python\n",
    "mean_Pressure9am = weather_data[\"Pressure9am\"].mean()\n",
    "\n",
    "weather_data.fillna({\n",
    "    \"Pressure9am\": mean_Pressure9am,\n",
    "}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the missing values have been filled in by running the following code:\n",
    "\n",
    "```python\n",
    "weather_data[\"Pressure9am\"].isna().sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Dataframes\n",
    "\n",
    "It's very often that we retrieve data from multiple sources. For each source, we have data stored in a separate dataframe. Thus, it is crucial for us to know how to combine the dataframes.\n",
    "\n",
    "`pandas` has 3 core methods to perform this task: `concat()`, `merge()` and `join()`. \n",
    "- The `concat()` method is used to combine multiple dataframes into one. \n",
    "- The `merge()` method is used to combine two dataframes based on a common column. \n",
    "- The `join()` method is used to combine two dataframes based on a common column.\n",
    "\n",
    "Most of what `join()` can do is the same as `merge()`, so I'll only demonstrate the `merge()` method here.\n",
    "\n",
    "The `join()` method combines two dataframes on the **basis of their indices** whereas the `merge()` method allows us to specify columns in addition to the indices to join on.\n",
    "- You can read more about the `join()` method [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html).\n",
    "- You can read about the difference between `join()` and `merge()` [here](https://www.geeksforgeeks.org/what-is-the-difference-between-join-and-merge-in-pandas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various forms of join:\n",
    "1. `inner`: Keep only elements that exist in both dataframes.\n",
    "1. `outer`: Keep all elements from all dataframes.\n",
    "1. `left`: Keep all elements from the left dataframe.\n",
    "1. `right`: Keep all elements from the right dataframe.\n",
    "\n",
    "Visual representations of various join types:\n",
    "\n",
    "|Type of Join|Image|\n",
    "|:-:|:-:|\n",
    "|Inner|![image](../../images/inner_join.png)|\n",
    "|Outer|![image](../../images/outer_join.png)|\n",
    "|Left|![image](../../images/left_join.png)|\n",
    "|Right|![image](../../images/right_join.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're familiar with the different types of join now, we will then see how to combine dataframes with `pandas`.\n",
    "\n",
    "First, we'll create two dataframes:\n",
    "\n",
    "```python\n",
    "left_df = pd.DataFrame({\n",
    "    \"common_col\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"left_value\": [10, 20, 30, 40, 50, 60, 70, 80],\n",
    "})\n",
    "\n",
    "right_df = pd.DataFrame({\n",
    "    \"common_col\": [3, 4, 5, 6, 9, 10, 11],\n",
    "    \"right_value\": [-30, -40, -50, -60, -90, -100, -110],\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `left_df` and `right_df` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inner` join `left_df` and `right_df` on the `common_col` column.\n",
    "\n",
    "```python\n",
    "inner_join_df = left_df.merge(right_df, on=\"common_col\", how=\"inner\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `inner_join_df` dataframe contains **only common values that exist in both** `left_df` and `right_df` dataframes. \n",
    "\n",
    "`outer` join `left_df` and `right_df`.\n",
    "\n",
    "```python\n",
    "outer_join_df = left_df.merge(right_df, on=\"common_col\", how=\"outer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `outer_join_df` dataframe contains **all common values that exist in either** `left_df` or `right_df` dataframes. \n",
    "\n",
    "`left` join `left_df` and `right_df`.\n",
    "\n",
    "```python\n",
    "left_join_df = left_df.merge(right_df, on=\"common_col\", how=\"left\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `left_join_df` contains all values from `left_df` and **only the common values that exist in both** `left_df` and `right_df` dataframes.\n",
    "\n",
    "`right` join `left_df` and `right_df`.\n",
    "\n",
    "```python\n",
    "right_join_df = left_df.merge(right_df, on=\"common_col\", how=\"right\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `right_join_df` contains all values from `right_df` and **only the common values that exist in both** `left_df` and `right_df` dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenation** is a bit different from the merging techniques that you saw above. With concatenation, we are stitching dataframes together along an axis - either horizontally or vertically.\n",
    "\n",
    "Visually, this is what concatenation looks like.\n",
    "\n",
    "|Type of concat|Axis|Image|\n",
    "|:-:|:-:|:-:|\n",
    "|`verical`|`0` (`row axis`)|![image](../../images/concat_vertical_axis0.png)|\n",
    "|`horizontal`|`1` (`column axis`)|![image](../../images/concat_horizontal_axis1.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see `concat()` in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e720081de2fd57dd700935953acd9dd6610ea2e1d5e7379bea03675c6c751eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
