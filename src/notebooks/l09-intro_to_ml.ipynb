{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Machine_learning), **Machine learning (ML)** is a field of inquiry devoted to understanding and building methods that `learn`, that is, methods that leverage data to improve performance on some set of tasks.\n",
    "\n",
    "**Why Machine Learning is important to you?**\n",
    "\n",
    "*TLDR*: Companies pay lots of money for people with Data Science/Machine Learning skills because Machine Learning is extremely important to them.\n",
    "\n",
    "Let's take a look at how much a Data Analyst/Data Scientist earn!\n",
    "\n",
    "|Job Title|Source|Salary|\n",
    "|:-:|:-:|:-:|\n",
    "|Data Analyst|[seek.com.au](https://www.seek.com.au/career-advice/role/data-analyst/salary)|![image](../../images/data_analyst_salary.png)|\n",
    "|Data Scientist|[seek.com.au](https://www.seek.com.au/career-advice/role/data-scientist/salary)|![image](../../images/data_scientist_salary.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI/ML History Recap\n",
    "\n",
    "**AI/ML Timeline**\n",
    "\n",
    "![image](../../images/ai_ml_history.png)\n",
    "\n",
    "|Period|Events|\n",
    "|:-|:-|\n",
    "|1950s|Statistical methods are discovered and refined. Pioneering machine learning research is conducted using simple algorithms.|\n",
    "|1960s|Bayesian methods are introduced for probabilistic inference in machine learning.|\n",
    "|1970s|'AI Winter' caused by pessimism about machine learning effectiveness.|\n",
    "|1980s|Rediscovery of backpropagation causes a resurgence in machine learning research.|\n",
    "|1990s|Work on Machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions â€“ or \"learn\" â€“ from the results. Support-vector machines (SVMs) and recurrent neural networks (RNNs) become popular. The fields of computational complexity via neural networks and super-Turing computation started.|\n",
    "|2000s|Support-Vector Clustering and other kernel methods and unsupervised machine learning methods become widespread.|\n",
    "|2010s|Deep learning becomes feasible, which leads to machine learning becoming integral to many widely used software services and applications.|\n",
    "|2016-2019|At OpenAI, weâ€™ve used the multiplayer video game Dota 2 as a research platform for general-purpose AI systems. Our Dota 2 AI, called OpenAI Five, learned by playing over 10,000 years of games against itself. It demonstrated the ability to achieve expert-level performance, learn humanâ€“AI cooperation, and operate at internet scale.|\n",
    "|In 2019|AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning|\n",
    "|In 2021|Protein Structure Prediction (AlphaFold 2) using Deep Learning|\n",
    "\n",
    "**Google AlphaGo 2016**\n",
    "\n",
    "![image](../../images/google_alphago.jpg)\n",
    "\n",
    "**OpenAI Dota 2**\n",
    "\n",
    "![image](../../images/openai_dota2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Learning\n",
    "\n",
    "There are three main types of machine learning:\n",
    "- **Supervised Machine Learning**: This is where you have a dataset and you know what the correct answer is.\n",
    "- **Unsupervised Machine Learning**: This is where you have a dataset and you don't know what the correct answer is.\n",
    "- **Reinforcement Learning**: The goal of reinforcement learning is to train an agent to complete a task within an uncertain environment. The agent receives observations and a reward from the environment and sends actions to the environment. The reward measures how successful action is with respect to completing the task goal.\n",
    "\n",
    "|Types of ML|Problems|Image|\n",
    "|:-:|:-|:-:|\n",
    "|Supervised|<ul><li>Classification</li><li>Regression</li></ul>|![image](../../images/supervised_learning.png)|\n",
    "|Unsupervised|<ul><li>Clustering</li><li>Dimensionality Reduction</li></ul>|![image](../../images/unsupervised.png)|\n",
    "|Reinforcement Learning|<ul><li>Navigation</li></ul>|![image](../../images/reinforcement.png)|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Machine Learning Project Life Cycle\n",
    "\n",
    "Generally, a Machine Learning project is broken down into the following steps:\n",
    "1. Data collection: from different sources could be internal and/or external to satisfy the business requirements/problems. Data could be in any format. CSV, XML.JSON, etc., here Big Data is playing a vital role to make sure the right data is in the expected format and structure.\n",
    "1. Exploratory Data Analysis (EDA): gather insights from the dataset. Identify the necessary feature engineering steps to be done in the next step.\n",
    "1. Data preparation: The data is prepared for the ML task. This preparation involves data cleaning, where you split the data into training, validation, and test sets (70-10-20 or 70-15-15 are common ratios). You also apply data transformations and feature engineering to the model that solves the target task. The output of this step are the data splits in the prepared format.\n",
    "1. Model training: The data scientist implements different algorithms with the prepared data to train various ML models. In addition, you subject the implemented algorithms to hyperparameter tuning to get the best performing ML model. The output of this step is a trained model.\n",
    "1. Model evaluation: The model is evaluated on a holdout test set to evaluate the model quality. Evaluate the model against a set of metrics like: AUC, Confusion Matrix, Accuracy, RMSE, R2, etc.\n",
    "1. Model validation: The model is confirmed to be adequate for deploymentâ€”that its predictive performance is better than a certain baseline.\n",
    "1. Deployment: means the integration of the finalized model into a production environment and getting results to make business decisions.\n",
    "1. Monitoring: The model predictive performance is monitored to potentially invoke a new iteration in the ML process.\n",
    "\n",
    "![image](../../images/ml_project_lifecycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Algorithms Cheatsheet\n",
    "\n",
    "**SKLearn** is definitely one of the most popular Python libraries for Machine Learning. It is a Python package that provides a high-level interface to the machine learning algorithms in the scikit-learn library.\n",
    "\n",
    "When dealing with a business problem, you need to be able to translate the problem into an ML problem. Next, you'll need to decide which ML algorithm(s) to use to solve your problem. The following [map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is probably a good starting point for you.\n",
    "\n",
    "![image](../../images/sklearn_ml_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project\n",
    "\n",
    "As part of this course, we will be building a **supervised machine learning model** to **predict whether a person makes over $50k USD a year**. \n",
    "\n",
    "**NOTE**: I have already downloaded the dataset and converted it to `.csv` format. You can find the dataset in the `data` folder.\n",
    "```bash\n",
    ".\n",
    "â”œâ”€â”€ census.csv\n",
    "â”œâ”€â”€ OnlineRetail.csv\n",
    "â””â”€â”€ weatherAUS.csv\n",
    "```\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "[Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income)\n",
    "\n",
    "Each row represents a person in the dataset. \n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "- `age`: continuous.\n",
    "- `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- `fnlwgt`: continuous.\n",
    "`education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, - 5th-6th, Preschool.\n",
    "- `education-num`: continuous.\n",
    "- `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "`occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, - Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- `relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- `sex`: Female, Male.\n",
    "- `capital-gain`: continuous.\n",
    "- `capital-loss`: continuous.\n",
    "- `hours-per-week`: continuous.\n",
    "- `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, - Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "- `income`: >50K, <=50K.\n",
    "\n",
    "***\n",
    "\n",
    "Since we're trying to predict whether a person makes over $50k USD a year, `income` is our target variable, while the other attributes are features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Similar to what we learned in the last lesson, we first start by importing the necessary packages for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# visualisation\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# better matplotlib plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format=\"svg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you load the `census.csv` file into a pandas dataframe and save it as `data`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you use `shape` to check the shape of the data and print out the number of **rows** and **columns** in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you use `head` to print out the first 5 rows of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's do some basic data exploration to understand the data.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Can you use `describe` to print out the basic statistics for both numerical and categorical data in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Do you see any `null` values in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. What's the number of individuals having `income` less than or equal to `50,000` USD? Store this value in `num_le_50k`.\n",
    "1. What's the number of individuals having `income` greater than `50,000` USD? Store this value as a variable called `num_gt_50k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "What's the count of `Female` and `Male` for each `income` bracket?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result obtained from the previous exercise, it appears that if you're a woman, you're less likely to have `income` above $`50k`.\n",
    "\n",
    "Let's make this clearer by visualising the data as a stacked bar chart and percentage stacked bar chart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of Male/Female by income bracket\n",
    "\n",
    "\n",
    "# percentage stacked bar chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "**HARD** ðŸ¤¯\n",
    "\n",
    "Can you use what you learnt above to explore if there's any relationship between `race` and `income`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Valid-Test Split\n",
    "\n",
    "**Train-Valid-Test Split** is the process of splitting the original dataset into 3 separate datasets: **train**, **valid**, and **test**. More about this can be found [here](https://www.youtube.com/watch?v=1waHlpKiNyY).\n",
    "- The **train** dataset is used to train the model. Normally, the **train** dataset accounts for **70%** of the data.\n",
    "- The **valid** dataset is used to validate the model. Normally, the **valid** dataset accounts for **10%** of the data.\n",
    "- The **test** dataset is used to test the model. Normally, the **test** dataset accounts for **20%** of the data.\n",
    "\n",
    "The purpose of separating the dataset into train, valid, and test is to prevent overfitting. We **ABSOLUTELY CANNOT** allow the model to learn from the **test** dataset because if it's the case, we will have a **CRAZILY GOOD** model that cannot generalise to new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to split the dataset is to use the `train_test_split` function in `sklearn.model_selection`.\n",
    "\n",
    "Let's split the data into 80% **train** and 20% **test**.\n",
    "\n",
    "```python\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=40, stratify=data['income'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you further split the **train** dataset into **train** and **valid** dataset such that the remaining **train** dataset accounts for 70% of the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the final shape of the **train**, **valid** and **test** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the **train**, **valid** and **test** dataset respectively account for 70%, 10%, and 20% of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Feature_engineering), **feature engineering** or feature extraction or feature discovery is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process.\n",
    "\n",
    "In other words, before the data can be fed as input for the machine learning algorithm, it needs to be cleaned, formatted and transformed into a form that the algorithm can understand. Fortunately, the dataset we have doesn't contain any missing entries. Nonetheless, there are still some steps we need to do.\n",
    "\n",
    "### Numerical Features\n",
    "\n",
    "A dataset may contain one or many features whose values are highly skewed. Algorithms can be sensitive to such distributions of values and may fail to perform well. It is very common to transform the skewed features into a more normal distribution.\n",
    "\n",
    "**QUICK RECAP**:\n",
    "- [Skewness](https://en.wikipedia.org/wiki/Skewness)\n",
    "- [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you define a function called `plot_histogram()` that satisfies the following requirements?\n",
    "- Input arguments: a dataframe and a column name of that dataframe.\n",
    "    - The column contains only numerical values.\n",
    "- Output: a histogram of the values in the provided column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exericse**\n",
    "\n",
    "Use your `plot_histogram()` function to find all skewed numerical features in the **train** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's is common practice to use a **logarithmic transformation** to transform skewed data into a more normal distribution. Doing so ensures that the very large and very small values do not negatively affect the model. \n",
    "\n",
    "**NOTE**: Since the logarithm of `0` is undefined, we must translate the value to a non-zero value by adding a small value to it before applying the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of skewed features\n",
    "skewed_features = [\"capital_gain\", \"capital_loss\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Use your `plot_histogram()` function to visualise the log-transformed features in the **train** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to transforming highly skewed features, it is often good practice to perform **some type of scaling on numerical features**. This is because some learning algorithms can be highly biased towards very large features and ignore the small ones if the data is not scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our scaler\n",
    "\n",
    "# fit our scaler to the numerical features of the processed training data\n",
    "\n",
    "\n",
    "# scale the numerical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, machine learning models can only handle numerical input data. Since our dataset contains lots of non-numeric data, we need to convert the categorical data into numerical data. What we are learning today is a very common technique called **one-hot encoding**. More about it is available [here](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "In essence, **one-hot encoding** creats a new feature for each possible value of the categorical variable. For example, there are 5 possible values (`White`, `Black`, `Asian-Pac-Islander`, `Amer-Indian-Eskimo` and `Other`) in the `race` column. Using **one-hot encoding**, we'll create 5 additional columns. Each of these columns will either be `0` or `1` depending on whether the value of the `race` column. The 5 additional columns are as below.\n",
    "- `race_White`\n",
    "- `race_Black`\n",
    "- `race_Asian-Pac-Islander`\n",
    "- `race_Amer-Indian-Eskimo`\n",
    "- `race_Other`\n",
    "\n",
    "**Original data:**\n",
    "|index|race|\n",
    "|:-:|:-:|\n",
    "|0|White|\n",
    "|1|Black|\n",
    "|2|Asian-Pac-Islander|\n",
    "|3|Amer-Indian-Eskimo|\n",
    "|4|Other|\n",
    "\n",
    "**One-hot encoded data:**\n",
    "\n",
    "|index|race|race_White|race_Black|race_Asian-Pac-Islander|race_Amer-Indian-Eskimo|race_Other|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|0|White|1|0|0|0|0|\n",
    "|1|Black|0|1|0|0|0|\n",
    "|2|Asian-Pac-Islander|0|0|1|0|0|\n",
    "|3|Amer-Indian-Eskimo|0|0|0|1|0|\n",
    "|4|Other|0|0|0|0|1|\n",
    "\n",
    "**NOTE**:\n",
    "- Sometimes, we can choose to have only `n-1` new columns for `n` unique values because the last value can be denoted by `n-1` `0`'s. However, as noted [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), **dropping one category** breaks the symmetry of the original representation and can therefore induce a bias in downstream models, for instance for penalized linear classification or regression models.\n",
    "\n",
    "Let's create our One-Hot Encoder using `sklearn.preprocessing.OneHotEncoder`.\n",
    "\n",
    "```python\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode the `race` column to demonstrate.\n",
    "\n",
    "```python\n",
    "# fit the encoder to the train data\n",
    "encoder.fit(train[[\"race\"]])\n",
    "\n",
    "# one-hot encoded the data\n",
    "ohe_example = encoder.transform(train[[\"race\"]])\n",
    "\n",
    "# save the result as a dataframe\n",
    "ohe_example = pd.DataFrame(\n",
    "    ohe_example,\n",
    "    columns=encoder.get_feature_names_out([\"race\"])\n",
    ")\n",
    "```\n",
    "\n",
    "**NOTE**:\n",
    "OneHotEncoder returns a `np.ndarray`. For this demo, we have created a corresponding dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the encoder to the train data\n",
    "\n",
    "\n",
    "# one-hot encoded the data\n",
    "\n",
    "\n",
    "# save the result as a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 5 rows of the `ohe_example` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we have one-hot encoded the `race` column correctly by checking if the number of `Black` in the `train` dataset is the same as that of `ohe_example` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode all categorical features by following these steps!\n",
    "1. Define our encoder.\n",
    "1. Save all categorical columns in a list.\n",
    "1. Fit the encoder to the train data.\n",
    "1. Transform the train data.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Can you define a `OneHotEncoder` object and assign it to `ohe`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you find all categorical columns in the `train` dataset and assign them to `categorical_features`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you fit the encoder to the categorical columns in the `train` dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "- Can you transform the categorical features in the `train`, `valid` and `test` datasets?\n",
    "- Save the results from the prior step as `ohe_train`, `ohe_valid` and `ohe_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you concatenate the **one-hot encoded** datasets to the respective **processed** datasets?\n",
    "\n",
    "For example, you should concatenate `ohe_train` with `processed_train` to get the final `processed_train` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Since our target variable (`income`) is non-numeric, can you convert it to numerical data?\n",
    "- `>50K`: `1`\n",
    "- `<=50K`: `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have succesfully created a dataset that can be used by our model.\n",
    "\n",
    "One final step is to drop the **original** categorical columns from the **processed** datasets.\n",
    "\n",
    "```python\n",
    "processed_train.drop(columns=categorical_features, inplace=True)\n",
    "processed_valid.drop(columns=categorical_features, inplace=True)\n",
    "processed_test.drop(columns=categorical_features, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the list of features that we will be feeding to the model as `features`.\n",
    "\n",
    "```python\n",
    "features = [c for c in processed_train.columns if c != \"income\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Since our target variable (`income`) only has 2 values, this is a **binary classification problem**. There are **many algorithms** that can be used to solve this problem. \n",
    "\n",
    "We will be using 2 algorithms to solve this problem and evaluate them.\n",
    "\n",
    "|Algorithm|Quick Recap (Videos from **StatQuest with Josh Starmer** youtuber|\n",
    "|:-:|:-|\n",
    "|Logistic Regression|[Link](https://youtu.be/yIYKR4sgzI8)|\n",
    "|Random Forest|<ul><li>[Link Part 1](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)</li><li>[Link Part 2](https://www.youtube.com/watch?v=sQ870aTKqiM)</li></ul>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise the model\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Generate prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise the model\n",
    "\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "\n",
    "# Generate prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Depending on the ML problem, there are different metrics to evaluate model performance. In the table below, you'll find the most basic metrics.\n",
    "\n",
    "|Metric|Description|Formula|\n",
    "|:-:|:-|:-:|\n",
    "|**True Positive**|A test result that correctly indicates the presence of a condition or characteristic|$$TP = (Actual == 1) \\cap (Predicted == 1)$$|\n",
    "|**True Negative**|A test result that correctly indicates the absence of a condition or characteristic|$$TN = (Actual == 0) \\cap (Predicted == 0)$$|\n",
    "|**False Positive**|A test result which wrongly indicates that a particular condition or attribute is present|$$FP = (Actual == 0) \\cap (Predicted == 1)$$|\n",
    "|**False Negative**|A test result which wrongly indicates that a particular condition or attribute is absent|$$FN = (Actual == 1) \\cap (Predicted == 0)$$|\n",
    "|**Accuracy**|Accuracy measures how often the model makes correct prediction|$$\\frac {TP + TN}{TP + FP + TN + FN}$$|\n",
    "|**Precision**|Precision measures the percentage of positive predictions that are actually correct|$$\\frac {TP}{TP + FP}$$|\n",
    "|**Recall**|Recall measures the percentage of actualy positives that are identified correctly|$$\\frac {TP}{TP + FN}$$|\n",
    "|**F1 Score**|F1 Score is the harmonic mean of Precision and Recall|$$\\frac {2TP}{2TP + FP + FN}$$|\n",
    "|**Specificity**|Specificity (or True Negative Rate) measures the percentage of negatives that are actually absent|$$\\frac {TN}{TN + FP}$$|\n",
    "\n",
    "\n",
    "**Confusion matrix** is a table that provides insight into the performance of a model. \n",
    "\n",
    "![image](../../images/confusion_matrix.webp)\n",
    "\n",
    "For classification problems that are skewed towards one class, **accuracy** is **NOT** a good metric to use. \n",
    "\n",
    "For example, if you have 100 samples and the ratio between the number of positive and negative samples is 9:1. A naive model that always predicts positive will have an accuracy of `90%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our models' performance using a confusion matrix!\n",
    "\n",
    "```python\n",
    "# Logistic regression model\n",
    "print(confusion_matrix(processed_test[\"income\"], processed_test[\"lr_predicted_income\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the values for **TN**, **FP**, **FN** and **TP** from the `sklearn.metrics.`confusion_matrix` function.\n",
    "\n",
    "```python\n",
    "tn, fp, fn, tp = confusion_matrix(processed_test[\"income\"], processed_test[\"lr_predicted_income\"]).ravel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Based on the values of **TN**, **FP**, **FN** and **TP**, can you calculate the accuracy, precision, recall and F1 score of the Logistic Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the results are calculated correctly by using `sklearn.metrics.accuracy_score`, `sklearn.metrics.precision_score`, `sklearn.metrics.recall_score` and `sklearn.metrics.f1_score`.\n",
    "\n",
    "```python\n",
    "# Logistic regression model\n",
    "print(f\"Accuracy : {accuracy_score(processed_test['income'], processed_test['lr_predicted_income']):,.2f}\")\n",
    "print(f\"Precision: {precision_score(processed_test['income'], processed_test['lr_predicted_income']):,.2f}\")\n",
    "print(f\"Recall   : {recall_score(processed_test['income'], processed_test['lr_predicted_income']):,.2f}\")\n",
    "print(f\"F1       : {f1_score(processed_test['income'], processed_test['lr_predicted_income']):,.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn.metrics.classification_report` to get a comprehensive report of the model performance. Read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). \n",
    "- `macro average`: averaging the unweighted mean per label.\n",
    "- `weighted average`: averaging the support-weighted mean per label.\n",
    "\n",
    "Let's get the classification report for our Logistic Regression model!\n",
    "\n",
    "```python\n",
    "print(classification_report(processed_test[\"income\"], processed_test[\"lr_predicted_income\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you get the confusion matrix for our Random Forest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you extract the values of **TN**, **FP**, **FN** and **TP** from the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you calculate the accuracy, precision, recall and F1 score for our Random Forest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you get the classification report for our Random Forest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, for classification problems, we often look at another metric called `AUC` to evaluate the model performance. Read more about it [here](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
    "\n",
    "An **ROC curve** is a plot of the true positive rate against the false positive rate.\n",
    "\n",
    "<img src=\"../../images/ROCCurve.svg\" width=\"600\" height=\"600\">\n",
    "\n",
    "*Image Source: Google ML Crash course*\n",
    "\n",
    "**AUC** is the area under the ROC curve.\n",
    "\n",
    "<img src=\"../../images/AUC.svg\" width=\"600\" height=\"600\">\n",
    "\n",
    "*Image Source: Google ML Crash course*\n",
    "\n",
    "We can use `sklearn.metrics.roc_auc_score` to calculate the AUC.\n",
    "\n",
    "```python\n",
    "print(f\"AUC (Train): {roc_auc_score(processed_train['income'], processed_train['lr_predicted_income'])}\")\n",
    "print(f\"AUC (Valid): {roc_auc_score(processed_valid['income'], processed_valid['lr_predicted_income'])}\")\n",
    "print(f\"AUC (Test) : {roc_auc_score(processed_test['income'], processed_test['lr_predicted_income'])}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you get the AUC for our Random Forest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "ML models have many hyperparameters that can be tuned to improve the model performance. This is a process called **hyperparameter tuning** (or **model optimisation**). The result of this process is a single set of well-performing parameters that can be used to configure a model.\n",
    "\n",
    "Read more about the basic steps to perform hyperparameter tuning [here](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/).\n",
    "\n",
    "\n",
    "![image](../../images/hyperparameter_tuning.png)\n",
    "\n",
    "*Image Source: Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. Journal of Machine Learning Research 13, 281â€“305 (2012)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a search space\n",
    "\n",
    "\n",
    "# create a grid search object\n",
    "\n",
    "\n",
    "# fit the grid search object to the training data\n",
    "\n",
    "\n",
    "# get the best model\n",
    "\n",
    "\n",
    "# make predictions using the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics from unoptimised model\n",
    "\n",
    "\n",
    "# print metrics from optimised model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e720081de2fd57dd700935953acd9dd6610ea2e1d5e7379bea03675c6c751eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
